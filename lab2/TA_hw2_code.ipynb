{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOQwPofNArr84CxVGWZcITv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jimmy93029/Selected_topic_for_RL_sophomore_fall/blob/master/lab2/TA_hw2_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### environment"
      ],
      "metadata": {
        "id": "6Lq6tzeSiUFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install gym==0.26.1\n",
        "%pip install \"gym[atari, accept-rom-license]\""
      ],
      "metadata": {
        "id": "58_rL3b6iU5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "from collections import deque\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from abc import ABC, abstractmethod\n",
        "from collections import deque\n",
        "import random\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import gym"
      ],
      "metadata": {
        "id": "L7Q0nakLiXMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ale_py\n",
        "from gym.wrappers import AtariPreprocessing\n",
        "# print(gym.envs.registry.keys())"
      ],
      "metadata": {
        "id": "4HixXfiYiZls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Atari Net network"
      ],
      "metadata": {
        "id": "jafnVETyhkW5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "class AtariNet(nn.Module):\n",
        "    def __init__(self, num_classes=4, init_weights=True):\n",
        "        super(AtariNet, self).__init__()\n",
        "\n",
        "        self.cnn = nn.Sequential(nn.Conv2d(4, 32, kernel_size=8, stride=4),\n",
        "                                        nn.ReLU(True),\n",
        "                                        nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "                                        nn.ReLU(True),\n",
        "                                        nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "                                        nn.ReLU(True)\n",
        "                                        )\n",
        "        self.action_logits = nn.Sequential(nn.Linear(7*7*64, 512),\n",
        "                                        nn.ReLU(True),\n",
        "                                        nn.Linear(512, num_classes)\n",
        "                                        )\n",
        "        self.value = nn.Sequential(nn.Linear(7*7*64, 512),\n",
        "                                        nn.ReLU(True),\n",
        "                                        nn.Linear(512, 1)\n",
        "                                        )\n",
        "\n",
        "        if init_weights:\n",
        "            self._initialize_weights()\n",
        "\n",
        "    def forward(self, x, eval=False, a=[]):\n",
        "        x = x.float() / 255.\n",
        "        x = self.cnn(x)\n",
        "        x = torch.flatten(x, start_dim=1)\n",
        "        value = self.value(x)\n",
        "        value = torch.squeeze(value)\n",
        "\n",
        "        logits = self.action_logits(x)\n",
        "        dist = Categorical(logits=logits)\n",
        "\n",
        "        if eval:\n",
        "            action = torch.argmax(logits, axis=1)\n",
        "        else:\n",
        "            action = dist.sample()\n",
        "\n",
        "        if len(a) == 0:\n",
        "            action_log_prob = dist.log_prob(action)\n",
        "        else:\n",
        "            action_log_prob = dist.log_prob(a)\n",
        "\n",
        "        dist_entropy = dist.entropy().mean()\n",
        "        action_log_prob = torch.squeeze(action_log_prob)\n",
        "        return action, action_log_prob, value, dist_entropy\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        # orthogonal initialization for PPO\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.orthogonal_(m.weight, np.sqrt(2))\n",
        "                nn.init.constant_(m.bias, 0.0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.orthogonal_(m.weight, np.sqrt(2))\n",
        "                nn.init.constant_(m.bias, 0.0)\n",
        "\n",
        "\n",
        "class AtariNetDQN(nn.Module):\n",
        "    def __init__(self, num_classes=4, init_weights=True):\n",
        "        super(AtariNetDQN, self).__init__()\n",
        "        self.cnn = nn.Sequential(nn.Conv2d(4, 32, kernel_size=8, stride=4),\n",
        "                                        nn.ReLU(True),\n",
        "                                        nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "                                        nn.ReLU(True),\n",
        "                                        nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "                                        nn.ReLU(True)\n",
        "                                        )\n",
        "        self.classifier = nn.Sequential(nn.Linear(7*7*64, 512),\n",
        "                                        nn.ReLU(True),\n",
        "                                        nn.Linear(512, num_classes)\n",
        "                                        )\n",
        "\n",
        "        if init_weights:\n",
        "            self._initialize_weights()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.float() / 255.\n",
        "        x = self.cnn(x)\n",
        "        x = torch.flatten(x, start_dim=1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0.0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1.0)\n",
        "                nn.init.constant_(m.bias, 0.0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                nn.init.constant_(m.bias, 0.0)\n"
      ],
      "metadata": {
        "id": "nnneaZNYhla7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RreplayBuffer"
      ],
      "metadata": {
        "id": "dVKkTDEMg5lM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from collections import deque\n",
        "import random\n",
        "\n",
        "class ReplayMemory(object):\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def append(self, *transition):\n",
        "        \"\"\"Saves a transition\"\"\"\n",
        "        self.buffer.append(tuple(map(tuple, transition)))\n",
        "\n",
        "    def sample(self, batch_size, device):\n",
        "        \"\"\"Sample a batch of transitions\"\"\"\n",
        "        transitions = random.sample(self.buffer, batch_size)\n",
        "        return (torch.tensor(np.asarray(x), dtype=torch.float, device=device) for x in zip(*transitions))"
      ],
      "metadata": {
        "id": "DixeKItGg6zf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DQNBaseAgent"
      ],
      "metadata": {
        "id": "03o0v_nigifi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "from collections import deque\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from replay_buffer.gae_replay_buffer import GaeSampleMemory\n",
        "from replay_buffer.replay_buffer import ReplayMemory\n",
        "from abc import ABC, abstractmethod\n",
        "\n",
        "class DQNBaseAgent(ABC):\n",
        "\tdef __init__(self, config):\n",
        "\t\tself.gpu = config[\"gpu\"]\n",
        "\t\tself.device = torch.device(\"cuda\" if self.gpu and torch.cuda.is_available() else \"cpu\")\n",
        "\t\tself.total_time_step = 0\n",
        "\t\tself.training_steps = int(config[\"training_steps\"])\n",
        "\t\tself.batch_size = int(config[\"batch_size\"])\n",
        "\t\tself.epsilon = 1.0\n",
        "\t\tself.eps_min = config[\"eps_min\"]\n",
        "\t\tself.eps_decay = config[\"eps_decay\"]\n",
        "\t\tself.eval_epsilon = config[\"eval_epsilon\"]\n",
        "\t\tself.warmup_steps = config[\"warmup_steps\"]\n",
        "\t\tself.use_double = config[\"use_double\"]\n",
        "\t\tself.eval_interval = int(2**16)\n",
        "\t\tself.eval_episode = 16\n",
        "\t\tself.num_envs = config[\"num_envs\"]\n",
        "\t\tself.gamma = config[\"gamma\"]\n",
        "\t\tself.update_freq = config[\"update_freq\"]\n",
        "\t\tself.update_target_freq = config[\"update_target_freq\"]\n",
        "\n",
        "\t\tself.replay_buffer = ReplayMemory(int(config[\"replay_buffer_capacity\"]))\n",
        "\t\tself.writer = SummaryWriter(config[\"logdir\"])\n",
        "\n",
        "\t@abstractmethod\n",
        "\tdef decide_agent_actions(self, observation, epsilon=0.0, action_space=None):\n",
        "\t\t# get action from behavior net, with epsilon-greedy selection\n",
        "\n",
        "\t\treturn NotImplementedError\n",
        "\n",
        "\tdef update(self):\n",
        "\t\tif self.total_time_step % self.update_freq == 0:\n",
        "\t\t\tself.update_behavior_network()\n",
        "\t\tif self.total_time_step % self.update_target_freq == 0:\n",
        "\t\t\tself.update_target_network()\n",
        "\n",
        "\tdef update_behavior_network(self):\n",
        "\t\t# sample a minibatch of transitions\n",
        "\t\tstate, action, reward, next_state, done = self.replay_buffer.sample(self.batch_size, self.device)\n",
        "\t\taction = action.type(torch.long)\n",
        "\t\tq_value = self.behavior_net(state).gather(1, action)\n",
        "\t\twith torch.no_grad():\n",
        "\t\t\tif self.use_double:\n",
        "\t\t\t\tq_next = self.behavior_net(next_state)\n",
        "\t\t\t\taction_index = q_next.max(dim=1)[1].view(-1, 1)\n",
        "\t\t\t\t# choose related Q from target net\n",
        "\t\t\t\tq_next = self.target_net(next_state).gather(dim=1, index=action_index.long())\n",
        "\t\t\telse:\n",
        "\t\t\t\tq_next = self.target_net(next_state).detach().max(1)[0].unsqueeze(1)\n",
        "\n",
        "\t\t\t# if episode terminates at next_state, then q_target = reward\n",
        "\t\t\tq_target = reward + self.gamma * q_next * (1 - done)\n",
        "\n",
        "\t\tcriterion = nn.SmoothL1Loss()\n",
        "\t\t# criterion = nn.MSELoss()\n",
        "\t\tloss = criterion(q_value, q_target)\n",
        "\n",
        "\t\tself.writer.add_scalar('DQN/Loss', loss.item(), self.total_time_step)\n",
        "\n",
        "\t\tself.optim.zero_grad()\n",
        "\t\tloss.backward()\n",
        "\t\t# nn.utils.clip_grad_norm_(self.behavior_net.parameters(), 5)\n",
        "\t\tfor param in self.behavior_net.parameters():\n",
        "\t\t\tparam.grad.data.clamp_(-1, 1)\n",
        "\n",
        "\t\tself.optim.step()\n",
        "\n",
        "\tdef update_target_network(self):\n",
        "\t\tself.target_net.load_state_dict(self.behavior_net.state_dict())\n",
        "\n",
        "\tdef epsilon_decay(self):\n",
        "\t\tself.epsilon -= (1 - self.eps_min) / self.eps_decay\n",
        "\t\tself.epsilon = max(self.epsilon, self.eps_min)\n",
        "\n",
        "\tdef train(self):\n",
        "\t\tself.behavior_net.train()\n",
        "\t\tobservations, infos = self.env.reset()\n",
        "\t\tepisode_rewards = [0] * self.num_envs\n",
        "\t\tepisode_lens = [0] * self.num_envs\n",
        "\n",
        "\t\twhile self.total_time_step <= self.training_steps:\n",
        "\t\t\tif self.total_time_step < self.warmup_steps:\n",
        "\t\t\t\tactions = self.decide_agent_actions(observations, 1.0, self.env.action_space.n)\n",
        "\t\t\telse:\n",
        "\t\t\t\tactions = self.decide_agent_actions(observations, self.epsilon, self.env.action_space.n)\n",
        "\t\t\t\tself.epsilon_decay()\n",
        "\n",
        "\t\t\tnext_observations, rewards, terminates, truncates, infos = self.env.step(actions)\n",
        "\n",
        "\t\t\tfor i in range(self.num_envs):\n",
        "\t\t\t\tself.replay_buffer.append(\n",
        "\t\t\t\t\t\tobservations[i],\n",
        "\t\t\t\t\t\t[actions[i]],\n",
        "\t\t\t\t\t\t[rewards[i]],\n",
        "\t\t\t\t\t\tnext_observations[i],\n",
        "\t\t\t\t\t\t[int(terminates[i])]\n",
        "\t\t\t\t\t)\n",
        "\n",
        "\t\t\tif self.total_time_step >= self.warmup_steps:\n",
        "\t\t\t\tself.update()\n",
        "\n",
        "\t\t\tepisode_rewards = [episode_rewards[i] + rewards[i] for i in range(self.num_envs)]\n",
        "\t\t\tepisode_lens = [episode_lens[i] + 1 for i in range(self.num_envs)]\n",
        "\n",
        "\t\t\tfor i in range(self.num_envs):\n",
        "\t\t\t\tif terminates[i] or truncates[i]:\n",
        "\t\t\t\t\tif i == 0:\n",
        "\t\t\t\t\t\tself.writer.add_scalar('Train/Episode Reward', episode_rewards[0], self.total_time_step)\n",
        "\t\t\t\t\t\tself.writer.add_scalar('Train/Episode Len', episode_lens[0], self.total_time_step)\n",
        "\t\t\t\t\tprint(f\"[{self.total_time_step}/{self.training_steps}]\\\n",
        "\t\t\t\t\t\t\\tenv {i} \\\n",
        "\t   \t\t\t\t\t\\tepisode reward: {episode_rewards[i]}\\\n",
        "\t\t\t\t\t\t\\tepisode len: {episode_lens[i]}\\\n",
        "\t\t\t\t\t\t\\tepsilon: {self.epsilon}\\\n",
        "\t\t\t\t\t\t\")\n",
        "\t\t\t\t\tepisode_rewards[i] = 0\n",
        "\t\t\t\t\tepisode_lens[i] = 0\n",
        "\n",
        "\t\t\tobservations = next_observations\n",
        "\t\t\tself.total_time_step += self.num_envs\n",
        "\n",
        "\t\t\tif self.total_time_step % self.eval_interval == 0:\n",
        "\t\t\t\t# save model checkpoint\n",
        "\t\t\t\tavg_score = self.evaluate()\n",
        "\t\t\t\tself.save(os.path.join(self.writer.log_dir, f\"model_{self.total_time_step}_{int(avg_score)}.pth\"))\n",
        "\t\t\t\tself.writer.add_scalar('Evaluate/Episode Reward', avg_score, self.total_time_step)\n",
        "\n",
        "\tdef evaluate(self):\n",
        "\t\tprint(\"==============================================\")\n",
        "\t\tprint(\"Evaluating...\")\n",
        "\t\tself.behavior_net.eval()\n",
        "\t\tepisode_rewards = [0] * self.eval_episode\n",
        "\t\tall_rewards = [0] * self.eval_episode\n",
        "\t\tall_done = [False] * self.eval_episode\n",
        "\t\tobservations, infos = self.test_env.reset()\n",
        "\t\twhile True:\n",
        "\t\t\tactions = self.decide_agent_actions(observations, self.eval_epsilon, self.test_env.action_space.n)\n",
        "\t\t\tnext_observations, rewards, terminates, truncates, infos = self.test_env.step(actions)\n",
        "\t\t\tfor i in range(self.eval_episode):\n",
        "\t\t\t\tif (terminates[i] or truncates[i]) and not all_done[i]:\n",
        "\t\t\t\t\tprint(f\"env {i} terminated, reward: {episode_rewards[i]}\")\n",
        "\t\t\t\t\tall_rewards[i] = episode_rewards[i]\n",
        "\t\t\t\t\tall_done[i] = True\n",
        "\n",
        "\t\t\tepisode_rewards = [episode_rewards[i] + rewards[i] for i in range(self.eval_episode)]\n",
        "\t\t\tobservations = next_observations\n",
        "\n",
        "\t\t\t# all episodes done, terminate\n",
        "\t\t\tif all(all_done):\n",
        "\t\t\t\tbreak\n",
        "\n",
        "\n",
        "\t\tavg = sum(all_rewards) / self.eval_episode\n",
        "\t\tprint(f\"average score: {avg}\")\n",
        "\t\tprint(\"==============================================\")\n",
        "\t\tself.behavior_net.train()\n",
        "\t\treturn avg\n",
        "\n",
        "\t# save model\n",
        "\tdef save(self, save_path):\n",
        "\t\ttorch.save(self.behavior_net.state_dict(), save_path)\n",
        "\n",
        "\t# load model\n",
        "\tdef load(self, load_path):\n",
        "\t\tself.behavior_net.load_state_dict(torch.load(load_path))\n",
        "\n",
        "\t# load model weights and evaluate\n",
        "\tdef load_and_evaluate(self, load_path):\n",
        "\t\tself.load(load_path)\n",
        "\t\tself.evaluate()\n"
      ],
      "metadata": {
        "id": "UXfIppgAghQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### AtariDQNAgent"
      ],
      "metadata": {
        "id": "B7NLZkQ_e854"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from base_agent import DQNBaseAgent\n",
        "from models.atari_model import AtariNetDQN, AtariNetDuelingDQN\n",
        "import envpool\n",
        "import random\n",
        "\n",
        "class AtariDQNAgent(DQNBaseAgent):\n",
        "\tdef __init__(self, config):\n",
        "\t\tsuper(AtariDQNAgent, self).__init__(config)\n",
        "\n",
        "\t\tself.env = envpool.make(config[\"env_id\"], env_type=\"gym\", num_envs=self.num_envs, episodic_life=True, reward_clip=True)\n",
        "\t\tself.test_env = envpool.make(config[\"env_id\"], env_type=\"gym\", num_envs=self.eval_episode, episodic_life=False, reward_clip=False)\n",
        "\t\tself.behavior_net = AtariNetDQN(self.env.action_space.n)\n",
        "\t\t#　self.behavior_net = AtariNetDuelingDQN(self.env.action_space.n)\n",
        "\t\tself.behavior_net.to(self.device)\n",
        "\t\tself.target_net = AtariNetDQN(self.env.action_space.n)\n",
        "\t\t# self.target_net = AtariNetDuelingDQN(self.env.action_space.n)\n",
        "\t\tself.target_net.to(self.device)\n",
        "\t\tself.target_net.load_state_dict(self.behavior_net.state_dict())\n",
        "\t\tself.target_net.eval()\n",
        "\t\tself.lr = config[\"learning_rate\"]\n",
        "\t\tself.optim = torch.optim.Adam(self.behavior_net.parameters(), lr=self.lr, eps=1.5e-4)\n",
        "\n",
        "\tdef decide_agent_actions(self, observation, epsilon=0.0, action_space=None):\n",
        "\t\tobservation = torch.from_numpy(observation)\n",
        "\t\tobservation = observation.to(self.device, dtype=torch.float32)\n",
        "\t\tif random.random() < epsilon:\n",
        "\t\t\taction = np.random.randint(0, action_space, size=observation.shape[0])\n",
        "\t\telse:\n",
        "\t\t\taction = self.behavior_net(observation).argmax(dim=1).cpu().numpy()\n",
        "\n",
        "\t\treturn action\n",
        "\n",
        "\tdef evaluate(self):\n",
        "\t\tprint(\"==============================================\")\n",
        "\t\tprint(\"Evaluating...\")\n",
        "\t\tself.behavior_net.eval()\n",
        "\t\tepisode_rewards = [0] * self.eval_episode\n",
        "\t\tall_rewards = [0] * self.eval_episode\n",
        "\t\tzero_reward_counter = [0] * self.eval_episode\n",
        "\t\tall_done = [False] * self.eval_episode\n",
        "\t\tobservations, infos = self.test_env.reset()\n",
        "\t\twhile True:\n",
        "\t\t\tactions = self.decide_agent_actions(observations, self.eval_epsilon, self.test_env.action_space.n)\n",
        "\t\t\t# breakout: help agent to fire\n",
        "\t\t\tfor i in range(self.eval_episode):\n",
        "\t\t\t\tif zero_reward_counter[i] > 200:\n",
        "\t\t\t\t\tactions[i] = 1\n",
        "\t\t\t\t\tzero_reward_counter[i] = 0\n",
        "\n",
        "\t\t\tnext_observations, rewards, terminates, truncates, infos = self.test_env.step(actions)\n",
        "\t\t\tfor i in range(self.eval_episode):\n",
        "\t\t\t\tif (terminates[i] or truncates[i]) and not all_done[i]:\n",
        "\t\t\t\t\tprint(f\"env {i} terminated, reward: {episode_rewards[i]}\")\n",
        "\t\t\t\t\tall_rewards[i] = episode_rewards[i]\n",
        "\t\t\t\t\tall_done[i] = True\n",
        "\n",
        "\t\t\tfor i in range(self.eval_episode):\n",
        "\t\t\t\tif rewards[i] == 0:\n",
        "\t\t\t\t\tzero_reward_counter[i] += 1\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tzero_reward_counter[i] = 0\n",
        "\n",
        "\t\t\tepisode_rewards = [episode_rewards[i] + rewards[i] for i in range(self.eval_episode)]\n",
        "\t\t\tobservations = next_observations\n",
        "\n",
        "\t\t\t# all episodes done, terminate\n",
        "\t\t\tif all(all_done):\n",
        "\t\t\t\tbreak\n",
        "\n",
        "\n",
        "\t\tavg = sum(all_rewards) / self.eval_episode\n",
        "\t\tprint(f\"average score: {avg}\")\n",
        "\t\tprint(\"==============================================\")\n",
        "\t\tself.behavior_net.train()\n",
        "\t\treturn avg"
      ],
      "metadata": {
        "id": "puvSQUvge65w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### main"
      ],
      "metadata": {
        "id": "Vb9xJOEHd83J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPrKxxfZdmXj"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "\n",
        "\tconfig = {\n",
        "\t\t\"gpu\": True,\n",
        "\t\t\"training_steps\": 1e8,\n",
        "\t\t\"gamma\": 0.99,\n",
        "\t\t\"batch_size\": 32,\n",
        "\t\t\"eps_min\": 0.1,\n",
        "\t\t\"warmup_steps\": 20000,\n",
        "\t\t\"eps_decay\": 1000000,\n",
        "\t\t\"eval_epsilon\": 0.01,\n",
        "\t\t\"use_double\": True,\n",
        "\t\t\"replay_buffer_capacity\": 100000,\n",
        "\t\t\"logdir\": 'log/dqn_eval/',\n",
        "\t\t\"update_freq\": 4,\n",
        "\t\t\"update_target_freq\": 10000,\n",
        "\t\t\"learning_rate\": 0.0000625,\n",
        "\t\t\"env_id\": 'ALE/MsPacman-v5',\n",
        "\t\t\"num_envs\": 4,\n",
        "\t}\n",
        "\n",
        "\tagent = AtariDQNAgent(config)\n",
        "\tagent.train()\n"
      ]
    }
  ]
}