{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "authorship_tag": "ABX9TyP6DkyBN2cX6pWxnrzUURqw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jimmy93029/Selected_topic_for_RL_sophomore_fall/blob/master/lab2_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 強化學習專論 lab2 Colab code"
      ],
      "metadata": {
        "id": "3tP4u6nFfXHs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "enviroment"
      ],
      "metadata": {
        "id": "uPu4wM_1gH09"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install \"gym[atari, accept-rom-license]\""
      ],
      "metadata": {
        "id": "fsnPrYcLiRIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "from collections import deque\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from abc import ABC, abstractmethod"
      ],
      "metadata": {
        "id": "9wMunz7IgO_D"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import deque\n",
        "import random"
      ],
      "metadata": {
        "id": "RJ9OzqcugGnq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import gym"
      ],
      "metadata": {
        "id": "fS_h6RNUh63J"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ale_py\n",
        "\n",
        "print(gym.envs.registry.keys())"
      ],
      "metadata": {
        "id": "6vBmv4q3kAmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Tools**"
      ],
      "metadata": {
        "id": "FXfDmDTHfRLz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DQN 模型"
      ],
      "metadata": {
        "id": "mgC7RzCmf4gd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "P-qBFizwfLv8"
      },
      "outputs": [],
      "source": [
        "class AtariNetDQN(nn.Module):\n",
        "    def __init__(self, num_classes=9, init_weights=True):\n",
        "        super(AtariNetDQN, self).__init__()\n",
        "        self.cnn = nn.Sequential(nn.Conv2d(3, 32, kernel_size=8, stride=4),\n",
        "                                        nn.ReLU(True),\n",
        "                                        nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "                                        nn.ReLU(True),\n",
        "                                        nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "                                        nn.ReLU(True)\n",
        "                                        )\n",
        "        self.classifier = nn.Sequential(nn.Linear(7*7*64, 512),\n",
        "                                        nn.ReLU(True),\n",
        "                                        nn.Linear(512, num_classes)\n",
        "                                        )\n",
        "\n",
        "        if init_weights:\n",
        "            self._initialize_weights()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.tensor(x)\n",
        "        x = x.permute(0, 3, 1, 2)\n",
        "        x = x.float() / 255.\n",
        "        x = self.cnn(x)\n",
        "        x = torch.flatten(x, start_dim=1)  # 把每個 batch 裡面的 x flatten\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0.0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1.0)\n",
        "                nn.init.constant_(m.bias, 0.0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                nn.init.constant_(m.bias, 0.0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### reply buffer"
      ],
      "metadata": {
        "id": "ytEIEzlof8pm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayMemory(object):\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def append(self, *transition):\n",
        "        \"\"\"Saves a transition\"\"\"\n",
        "        self.buffer.append(tuple(map(tuple, transition)))\n",
        "\n",
        "    def sample(self, batch_size, device):\n",
        "        \"\"\"Sample a batch of transitions\"\"\"\n",
        "        transitions = random.sample(self.buffer, batch_size)\n",
        "        return (torch.tensor(np.asarray(x), dtype=torch.float, device=device) for x in zip(*transitions))"
      ],
      "metadata": {
        "id": "mx4exAHTgEzc"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "&emsp;"
      ],
      "metadata": {
        "id": "9NfEqNSXgpnG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Agents**"
      ],
      "metadata": {
        "id": "2U-WxZjjgYcI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### base agent"
      ],
      "metadata": {
        "id": "V8sRl31hgzZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DQNBaseAgent(ABC):\n",
        "\tdef __init__(self, config):\n",
        "\t\tself.gpu = config[\"gpu\"]\n",
        "\t\tself.device = torch.device(\"cuda\" if self.gpu and torch.cuda.is_available() else \"cpu\")\n",
        "\t\tself.total_time_step = 0\n",
        "\t\tself.training_steps = int(config[\"training_steps\"])\n",
        "\t\tself.batch_size = int(config[\"batch_size\"])\n",
        "\t\tself.epsilon = 1.0\n",
        "\t\tself.eps_min = config[\"eps_min\"]\n",
        "\t\tself.eps_decay = config[\"eps_decay\"]\n",
        "\t\tself.eval_epsilon = config[\"eval_epsilon\"]\n",
        "\t\tself.warmup_steps = config[\"warmup_steps\"]\n",
        "\t\tself.eval_interval = config[\"eval_interval\"]\n",
        "\t\tself.eval_episode = config[\"eval_episode\"]\n",
        "\t\tself.gamma = config[\"gamma\"]\n",
        "\t\tself.update_freq = config[\"update_freq\"]\n",
        "\t\tself.update_target_freq = config[\"update_target_freq\"]\n",
        "\n",
        "\t\tself.replay_buffer = ReplayMemory(int(config[\"replay_buffer_capacity\"]))\n",
        "\t\tself.writer = SummaryWriter(config[\"logdir\"])\n",
        "\n",
        "\t@abstractmethod\n",
        "\tdef decide_agent_actions(self, observation, epsilon=0.0, action_space=None):\n",
        "\t\t### TODO ###\n",
        "\t\t# get action from behavior net, with epsilon-greedy selection\n",
        "\n",
        "\t\treturn NotImplementedError\n",
        "\n",
        "\tdef update(self):\n",
        "\t\tif self.total_time_step % self.update_freq == 0:\n",
        "\t\t\tself.update_behavior_network()\n",
        "\t\tif self.total_time_step % self.update_target_freq == 0:\n",
        "\t\t\tself.update_target_network()\n",
        "\n",
        "\t@abstractmethod\n",
        "\tdef update_behavior_network(self):\n",
        "\t\t# sample a minibatch of transitions\n",
        "\t\tstate, action, reward, next_state, done = self.replay_buffer.sample(self.batch_size, self.device)\n",
        "\t\t### TODO ###\n",
        "\t\t# calculate the loss and update the behavior network\n",
        "\n",
        "\tdef update_target_network(self):\n",
        "\t\tself.target_net.load_state_dict(self.behavior_net.state_dict())\n",
        "\n",
        "\tdef epsilon_decay(self):\n",
        "\t\tself.epsilon -= (1 - self.eps_min) / self.eps_decay\n",
        "\t\tself.epsilon = max(self.epsilon, self.eps_min)\n",
        "\n",
        "\tdef train(self):\n",
        "\t\tepisode_idx = 0\n",
        "\t\twhile self.total_time_step <= self.training_steps:\n",
        "\t\t\tseed = random.randint(1, 100000)\n",
        "\t\t\tobservation = self.env.reset(seed=seed)\n",
        "\t\t\tepisode_reward = 0\n",
        "\t\t\tepisode_len = 0\n",
        "\t\t\tepisode_idx += 1\n",
        "\t\t\twhile True:\n",
        "\t\t\t\tif self.total_time_step < self.warmup_steps:\n",
        "\t\t\t\t\taction = self.decide_agent_actions(observation, 1.0, self.env.action_space)\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\taction = self.decide_agent_actions(observation, self.epsilon, self.env.action_space)\n",
        "\t\t\t\t\tself.epsilon_decay()\n",
        "\n",
        "\t\t\t\tnext_observation, reward, terminate, info = self.env.step(action)\n",
        "\t\t\t\tself.replay_buffer.append(observation, [action], [reward], next_observation, [int(terminate)])\n",
        "\n",
        "\t\t\t\tif self.total_time_step >= self.warmup_steps:\n",
        "\t\t\t\t\tself.update()\n",
        "\n",
        "\t\t\t\tepisode_reward += reward\n",
        "\t\t\t\tepisode_len += 1\n",
        "\n",
        "\t\t\t\tif terminate:\n",
        "\t\t\t\t\tself.writer.add_scalar('Train/Episode Reward', episode_reward, self.total_time_step)\n",
        "\t\t\t\t\tself.writer.add_scalar('Train/Episode Len', episode_len, self.total_time_step)\n",
        "\t\t\t\t\tprint(f\"[{self.total_time_step}/{self.training_steps}]  episode: {episode_idx}  \\\n",
        "\t\t\t\t\tepisode reward: {episode_reward}  episode len: {episode_len}  epsilon: {self.epsilon}\")\n",
        "\t\t\t\t\tbreak\n",
        "\n",
        "\t\t\t\tobservation = next_observation\n",
        "\t\t\t\tself.total_time_step += 1\n",
        "\n",
        "\t\t\tif episode_idx % self.eval_interval == 0:\n",
        "\t\t\t\t# save model checkpoint\n",
        "\t\t\t\tavg_score = self.evaluate()\n",
        "\t\t\t\tself.save(os.path.join(self.writer.log_dir, f\"model_{self.total_time_step}_{int(avg_score)}.pth\"))\n",
        "\t\t\t\tself.writer.add_scalar('Evaluate/Episode Reward', avg_score, self.total_time_step)\n",
        "\n",
        "\tdef evaluate(self):\n",
        "\t\tprint(\"==============================================\")\n",
        "\t\tprint(\"Evaluating...\")\n",
        "\t\tall_rewards = []\n",
        "\t\tfor i in range(self.eval_episode):\n",
        "\t\t\tobservation, info = self.test_env.reset()\n",
        "\t\t\ttotal_reward = 0\n",
        "\t\t\twhile True:\n",
        "\t\t\t\tself.test_env.render()\n",
        "\t\t\t\taction = self.decide_agent_actions(observation, self.eval_epsilon, self.test_env.action_space)\n",
        "\t\t\t\tnext_observation, reward, terminate, info = self.test_env.step(action)\n",
        "\t\t\t\ttotal_reward += reward\n",
        "\t\t\t\tif terminate:    # I remove truncated to solve error\n",
        "\t\t\t\t\tprint(f\"episode {i+1} reward: {total_reward}\")\n",
        "\t\t\t\t\tall_rewards.append(total_reward)\n",
        "\t\t\t\t\tbreak\n",
        "\n",
        "\t\t\t\tobservation = next_observation\n",
        "\n",
        "\n",
        "\t\tavg = sum(all_rewards) / self.eval_episode\n",
        "\t\tprint(f\"average score: {avg}\")\n",
        "\t\tprint(\"==============================================\")\n",
        "\t\treturn avg\n",
        "\n",
        "\t# save model\n",
        "\tdef save(self, save_path):\n",
        "\t\ttorch.save(self.behavior_net.state_dict(), save_path)\n",
        "\n",
        "\t# load model\n",
        "\tdef load(self, load_path):\n",
        "\t\tself.behavior_net.load_state_dict(torch.load(load_path))\n",
        "\n",
        "\t# load model weights and evaluate\n",
        "\tdef load_and_evaluate(self, load_path):\n",
        "\t\tself.load(load_path)\n",
        "\t\tself.evaluate()"
      ],
      "metadata": {
        "id": "sS6x1Om8gy1N"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DQN_Agent ( 作業重點 )"
      ],
      "metadata": {
        "id": "Xh8yg1UBg7u6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AtariDQNAgent(DQNBaseAgent):\n",
        "\tdef __init__(self, config):\n",
        "\t\tsuper(AtariDQNAgent, self).__init__(config)\n",
        "\n",
        "\t\tself.env = gym.make(config[\"env_id\"])\n",
        "\n",
        "\t\tself.test_env = gym.make(config[\"env_id\"])\n",
        "\n",
        "\t\t# initialize behavior network and target network\n",
        "\t\tself.behavior_net = AtariNetDQN(self.env.action_space.n)\n",
        "\t\tself.behavior_net.to(self.device)\n",
        "\t\tself.target_net = AtariNetDQN(self.env.action_space.n)\n",
        "\t\tself.target_net.to(self.device)\n",
        "\t\tself.target_net.load_state_dict(self.behavior_net.state_dict())\n",
        "\n",
        "\t\t# initialize optimizer\n",
        "\t\tself.lr = config[\"learning_rate\"]\n",
        "\t\tself.optim = torch.optim.Adam(self.behavior_net.parameters(), lr=self.lr, eps=1.5e-4)\n",
        "\n",
        "\tdef decide_agent_actions(self, observation, epsilon=0.0, action_space=None) -> int:\n",
        "\t\tif random.random() <= epsilon:\n",
        "\t\t\treturn random.randrange(self.env.action_space.n)\n",
        "\t\telse:\n",
        "\t\t\taction = self.behavior_net(observation).argmax().cpu().item()\n",
        "\t\t\treturn action\n",
        "\n",
        "\tdef choose_batch_actions(self, net, states) -> torch.Tensor:\n",
        "\t\tif net == \"behavior\":\n",
        "\t\t\t_, actions = self.behavior_net(states).max(dim=1, keepdim=True)\n",
        "\t\t\treturn actions\n",
        "\t\telif net == \"target\":\n",
        "\t\t\t_, actions = self.target_net(states).max(dim=1, keepdim=True)\n",
        "\t\t\treturn actions\n",
        "\n",
        "\tdef Q_value(self, net, states, actions):\n",
        "\t\t# get Q values from actions and target network\n",
        "\t\tif net == \"target\":\n",
        "\t\t\tnext_q_values = self.target_net.forward(states).gather(dim=1, index=actions)\n",
        "\t\t\treturn next_q_values\n",
        "\t\telif net == \"behavior\":\n",
        "\t\t\tq_values = self.behavior_net.forward(states).gather(dim=1, index=actions)\n",
        "\t\t\treturn q_values\n",
        "\n",
        "\t# implement DDQN\n",
        "\tdef update_behavior_network(self):\n",
        "\t\t# sample a minibatch of transitions\n",
        "\t\tstate, action, reward, next_state, done = self.replay_buffer.sample(self.batch_size, self.device)\n",
        "\n",
        "\t\tnext_actions = self.choose_batch_actions(\"behavior\", next_state)\n",
        "\t\tQ_target = reward + self.gamma * self.Q_value(\"target\", next_state, next_actions)\n",
        "\t\tQ_output = self.Q_value(\"behavior\", state, action)\n",
        "\n",
        "\t\tcriterion = nn.SmoothL1Loss()\n",
        "\t\tloss = criterion(Q_output, Q_target)\n",
        "\n",
        "\t\tself.writer.add_scalar('DQN/Loss', loss.item(), self.total_time_step)\n",
        "\n",
        "\t\tself.optim.zero_grad()\n",
        "\t\tloss.backward()\n",
        "\t\tself.optim.step()"
      ],
      "metadata": {
        "id": "_REgd9JQhI9z"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "&emsp;"
      ],
      "metadata": {
        "id": "j66ReT1rhdNb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## main"
      ],
      "metadata": {
        "id": "WvIN4EPXhe2B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    # my hyperparameters, you can change it as you like\n",
        "    config = {\n",
        "\t\t\"gpu\": True,\n",
        "\t\t\"training_steps\": 1e8,\n",
        "\t\t\"gamma\": 0.99,\n",
        "\t\t\"batch_size\": 32,\n",
        "\t\t\"eps_min\": 0.1,\n",
        "\t\t\"warmup_steps\": 20000,\n",
        "\t\t\"eps_decay\": 1000000,\n",
        "\t\t\"eval_epsilon\": 0.01,\n",
        "\t\t\"replay_buffer_capacity\": 100000,\n",
        "\t\t\"logdir\": 'log/DQN/',\n",
        "\t\t\"update_freq\": 4,\n",
        "\t\t\"update_target_freq\": 10000,\n",
        "\t\t\"learning_rate\": 0.0000625,\n",
        "        \"eval_interval\": 100,\n",
        "        \"eval_episode\": 5,\n",
        "\t\t\"env_id\": 'ALE/MsPacman-v5',\n",
        "\t}\n",
        "    agent = AtariDQNAgent(config)\n",
        "    agent.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zY8y_W7ShkYX",
        "outputId": "4b9f4f56-0bf1-453f-c71d-090dc48f85c2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  logger.deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[475/100000000]  episode: 1  \t\t\t\t\tepisode reward: 290.0  episode len: 476  epsilon: 1.0\n",
            "[1148/100000000]  episode: 2  \t\t\t\t\tepisode reward: 310.0  episode len: 674  epsilon: 1.0\n",
            "[1539/100000000]  episode: 3  \t\t\t\t\tepisode reward: 160.0  episode len: 392  epsilon: 1.0\n",
            "[2072/100000000]  episode: 4  \t\t\t\t\tepisode reward: 240.0  episode len: 534  epsilon: 1.0\n",
            "[2591/100000000]  episode: 5  \t\t\t\t\tepisode reward: 280.0  episode len: 520  epsilon: 1.0\n",
            "[3088/100000000]  episode: 6  \t\t\t\t\tepisode reward: 260.0  episode len: 498  epsilon: 1.0\n",
            "[3513/100000000]  episode: 7  \t\t\t\t\tepisode reward: 180.0  episode len: 426  epsilon: 1.0\n",
            "[4004/100000000]  episode: 8  \t\t\t\t\tepisode reward: 260.0  episode len: 492  epsilon: 1.0\n",
            "[4467/100000000]  episode: 9  \t\t\t\t\tepisode reward: 150.0  episode len: 464  epsilon: 1.0\n",
            "[4984/100000000]  episode: 10  \t\t\t\t\tepisode reward: 300.0  episode len: 518  epsilon: 1.0\n",
            "[5475/100000000]  episode: 11  \t\t\t\t\tepisode reward: 160.0  episode len: 492  epsilon: 1.0\n",
            "[5952/100000000]  episode: 12  \t\t\t\t\tepisode reward: 220.0  episode len: 478  epsilon: 1.0\n",
            "[6329/100000000]  episode: 13  \t\t\t\t\tepisode reward: 120.0  episode len: 378  epsilon: 1.0\n",
            "[6924/100000000]  episode: 14  \t\t\t\t\tepisode reward: 250.0  episode len: 596  epsilon: 1.0\n",
            "[7465/100000000]  episode: 15  \t\t\t\t\tepisode reward: 300.0  episode len: 542  epsilon: 1.0\n",
            "[7904/100000000]  episode: 16  \t\t\t\t\tepisode reward: 180.0  episode len: 440  epsilon: 1.0\n",
            "[8407/100000000]  episode: 17  \t\t\t\t\tepisode reward: 230.0  episode len: 504  epsilon: 1.0\n",
            "[8890/100000000]  episode: 18  \t\t\t\t\tepisode reward: 210.0  episode len: 484  epsilon: 1.0\n",
            "[9425/100000000]  episode: 19  \t\t\t\t\tepisode reward: 260.0  episode len: 536  epsilon: 1.0\n",
            "[9922/100000000]  episode: 20  \t\t\t\t\tepisode reward: 190.0  episode len: 498  epsilon: 1.0\n",
            "[10489/100000000]  episode: 21  \t\t\t\t\tepisode reward: 320.0  episode len: 568  epsilon: 1.0\n",
            "[10930/100000000]  episode: 22  \t\t\t\t\tepisode reward: 210.0  episode len: 442  epsilon: 1.0\n",
            "[11293/100000000]  episode: 23  \t\t\t\t\tepisode reward: 140.0  episode len: 364  epsilon: 1.0\n",
            "[11842/100000000]  episode: 24  \t\t\t\t\tepisode reward: 230.0  episode len: 550  epsilon: 1.0\n",
            "[12473/100000000]  episode: 25  \t\t\t\t\tepisode reward: 330.0  episode len: 632  epsilon: 1.0\n",
            "[12984/100000000]  episode: 26  \t\t\t\t\tepisode reward: 160.0  episode len: 512  epsilon: 1.0\n",
            "[13737/100000000]  episode: 27  \t\t\t\t\tepisode reward: 250.0  episode len: 754  epsilon: 1.0\n",
            "[14626/100000000]  episode: 28  \t\t\t\t\tepisode reward: 670.0  episode len: 890  epsilon: 1.0\n",
            "[15059/100000000]  episode: 29  \t\t\t\t\tepisode reward: 290.0  episode len: 434  epsilon: 1.0\n",
            "[15702/100000000]  episode: 30  \t\t\t\t\tepisode reward: 450.0  episode len: 644  epsilon: 1.0\n",
            "[16151/100000000]  episode: 31  \t\t\t\t\tepisode reward: 290.0  episode len: 450  epsilon: 1.0\n",
            "[16836/100000000]  episode: 32  \t\t\t\t\tepisode reward: 380.0  episode len: 686  epsilon: 1.0\n",
            "[17341/100000000]  episode: 33  \t\t\t\t\tepisode reward: 200.0  episode len: 506  epsilon: 1.0\n",
            "[17866/100000000]  episode: 34  \t\t\t\t\tepisode reward: 240.0  episode len: 526  epsilon: 1.0\n",
            "[18281/100000000]  episode: 35  \t\t\t\t\tepisode reward: 250.0  episode len: 416  epsilon: 1.0\n",
            "[18896/100000000]  episode: 36  \t\t\t\t\tepisode reward: 380.0  episode len: 616  epsilon: 1.0\n",
            "[19339/100000000]  episode: 37  \t\t\t\t\tepisode reward: 220.0  episode len: 444  epsilon: 1.0\n",
            "[19824/100000000]  episode: 38  \t\t\t\t\tepisode reward: 140.0  episode len: 486  epsilon: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-8872bd45c421>:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x = torch.tensor(x)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-730bae5dcf8b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \t}\n\u001b[1;32m     21\u001b[0m     \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAtariDQNAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-19-87c71061efc9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_time_step\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarmup_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m                                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                                 \u001b[0mepisode_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-87c71061efc9>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_time_step\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_freq\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_behavior_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_time_step\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_target_freq\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_target_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-95ba0db6c62a>\u001b[0m in \u001b[0;36mupdate_behavior_network\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                 \u001b[0mnext_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose_batch_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"behavior\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m                 \u001b[0mQ_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"target\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0mQ_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"behavior\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-95ba0db6c62a>\u001b[0m in \u001b[0;36mchoose_batch_actions\u001b[0;34m(self, net, states)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mchoose_batch_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"behavior\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m                         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbehavior_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m                         \u001b[0;32mreturn\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"target\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-8872bd45c421>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (32x22528 and 3136x512)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jqUptRrpmTOc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}